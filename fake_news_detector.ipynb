{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dataset = pd.read_csv('data/True.csv')\n",
    "fake_dataset = pd.read_csv('data/Fake.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import Normalizer, SentenceTokenizer, word_tokenize\n",
    "normalizer = Normalizer()\n",
    "sent_tokenizer = SentenceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "true_dataset['title'] = true_dataset.apply(lambda row: row['title'].lower(), axis=1)\n",
    "fake_dataset['title'] = fake_dataset.apply(lambda row: row['title'].lower(), axis=1)\n",
    "\n",
    "true_dataset['text'] = true_dataset.apply(lambda row: row['text'].lower(), axis=1)\n",
    "fake_dataset['text'] = fake_dataset.apply(lambda row: row['text'].lower(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dataset['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dataset['text'] = true_dataset.apply(lambda row: normalizer.normalize(row['text']), axis=1)\n",
    "\n",
    "true_dataset['title'] = true_dataset.apply(lambda row: normalizer.normalize(row['title']), axis=1)\n",
    "\n",
    "fake_dataset['text'] = fake_dataset.apply(lambda row: normalizer.normalize(row['text']), axis=1)\n",
    "\n",
    "fake_dataset['title'] = fake_dataset.apply(lambda row: normalizer.normalize(row['title']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_dataset.to_csv('data/TrueV1.csv', index=False)\n",
    "# fake_dataset.to_csv('data/FakeV1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_dataset = pd.read_csv('data/TrueV1.csv')\n",
    "# fake_dataset = pd.read_csv('data/FakeV1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(text):\n",
    "    return str(re.sub(r'\\d+', '', text))\n",
    "\n",
    "def remove_urls(text):\n",
    "    return str(re.sub(r'http[s]?://\\S+|www\\.\\S+', '', text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove num and url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dataset['text'] = true_dataset.apply(lambda row: remove_numbers(remove_urls(str(row['text']))), axis=1)\n",
    "\n",
    "true_dataset['title'] = true_dataset.apply(lambda row: remove_numbers(remove_urls(str(row['title']))), axis=1)\n",
    "\n",
    "fake_dataset['text'] = fake_dataset.apply(lambda row: remove_numbers(remove_urls(str(row['text']))), axis=1)\n",
    "\n",
    "fake_dataset['title'] = fake_dataset.apply(lambda row: remove_numbers(remove_urls(str(row['title']))), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove puncs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dataset['text'] = true_dataset.apply(lambda row: re.sub(r'[^\\w\\s]', '', row[\"text\"]), axis=1)\n",
    "\n",
    "true_dataset['title'] = true_dataset.apply(lambda row: re.sub(r'[^\\w\\s]', '', row[\"title\"]), axis=1)\n",
    "\n",
    "fake_dataset['text'] = fake_dataset.apply(lambda row: re.sub(r'[^\\w\\s]', '', row[\"text\"]), axis=1)\n",
    "\n",
    "fake_dataset['title'] = fake_dataset.apply(lambda row: re.sub(r'[^\\w\\s]', '', row[\"title\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_dataset.to_csv('data/TrueV2.csv', index=False)\n",
    "# fake_dataset.to_csv('data/FakeV2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_dataset['text'] = true_dataset.apply(lambda row: word_tokenize(row['text']), axis=1)\n",
    "\n",
    "# true_dataset['title'] = true_dataset.apply(lambda row: word_tokenize(row['title']), axis=1)\n",
    "\n",
    "# fake_dataset['text'] = fake_dataset.apply(lambda row: word_tokenize(row['text']), axis=1)\n",
    "\n",
    "# fake_dataset['title'] = fake_dataset.apply(lambda row: word_tokenize(row['title']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.text import Tokenizer # depracated\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=100,  # Maximum number of unique tokens\n",
    "    standardize='lower_and_strip_punctuation',  # Normalize text\n",
    "    output_sequence_length=3  # Output sequence length\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dataset = pd.read_csv('data/TrueV2.csv')\n",
    "fake_dataset = pd.read_csv('data/FakeV2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dataset['label'] = 1\n",
    "fake_dataset['label'] = 0\n",
    "dataset = pd.concat([true_dataset, fake_dataset], ignore_index=True)\n",
    "# dataset.to_csv('data/datasetV1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"text\"] = dataset[\"text\"].astype(str).apply(sent_tokenizer.tokenize)\n",
    "dataset[\"title\"] = dataset[\"title\"].astype(str).apply(sent_tokenizer.tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depracated\n",
    "# for series in [dataset['text'], dataset['title']]: \n",
    "#     for row in tqdm(series, total=series.shape[0], desc=\"Fitting tokenizer\"):\n",
    "#         tokenizer.fit_on_texts(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting tokenizer: 100%|██████████| 44898/44898 [01:50<00:00, 406.07it/s]\n",
      "Fitting tokenizer: 100%|██████████| 44898/44898 [01:40<00:00, 445.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# Adapt the layer to the data\n",
    "for series in [dataset['text'], dataset['title']]: \n",
    "    for row in tqdm(series, total=series.shape[0], desc=\"Fitting tokenizer\"):\n",
    "        vectorize_layer.adapt(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting text to tokens: 100%|██████████| 44898/44898 [01:45<00:00, 424.66it/s]\n",
      "Converting text to tokens: 100%|██████████| 44898/44898 [01:41<00:00, 442.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert text to tokens\n",
    "sentences_to_tokens_text = []\n",
    "sentences_to_token_title = []\n",
    "for series in tqdm(dataset['text'], total=dataset[\"text\"].shape[0], desc=\"Converting text to tokens\"): \n",
    "    for row in series:\n",
    "        sentences_to_tokens_text.append(vectorize_layer(row))\n",
    "\n",
    "for series in tqdm(dataset['title'], total=dataset[\"title\"].shape[0], desc=\"Converting text to tokens\"):\n",
    "    for row in series:\n",
    "        sentences_to_token_title.append(vectorize_layer(row))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"tokens_text\"] = sentences_to_tokens_text\n",
    "dataset[\"tokens_title\"] = sentences_to_token_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"text_sequences\"] = dataset[\"text\"].apply(vectorize_layer.texts_to_sequences)\n",
    "dataset[\"title_sequences\"] = dataset[\"title\"].apply(vectorize_layer.texts_to_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"padded_text_seqs\"] = dataset[\"text_sequences\"].apply(pad_sequences(padding='post'))\n",
    "dataset[\"padded_title_seqs\"] = dataset[\"title_sequences\"].apply(pad_sequences(padding='post'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('data/datasetV2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_pickle('data/datasetV2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

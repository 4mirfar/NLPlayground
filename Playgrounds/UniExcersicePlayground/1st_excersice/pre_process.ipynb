{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/hp_en.txt\", \"r\") as f:\n",
    "    hp_en = f.read()\n",
    "    \n",
    "with open(\"data/hp_fa.txt\", \"r\") as f:\n",
    "    hp_fa = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_en_no_space = re.sub(r'\\s+', ' ', hp_en).strip()\n",
    "hp_fa_no_space = re.sub(r'\\s+', ' ', hp_fa).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import Normalizer, SentenceTokenizer\n",
    "\n",
    "sentence_tokenizer = SentenceTokenizer()\n",
    "\n",
    "sentences_en = sentence_tokenizer.tokenize(hp_en_no_space)\n",
    "sentences_fa = sentence_tokenizer.tokenize(hp_fa_no_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "normalizer = Normalizer()\n",
    "\n",
    "\n",
    "normalized_sentences_en = [normalizer.normalize(sentence) for sentence in sentences_en]\n",
    "normalized_sentences_fa = [normalizer.normalize(sentence) for sentence in sentences_fa]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_sentences_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_en = [sentence.split(\" \") for sentence in normalized_sentences_en]\n",
    "words_fa = [sentence.split(\" \") for sentence in normalized_sentences_fa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_fa[110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_no_punctuation_en = []\n",
    "sents_no_punctuation_fa = []\n",
    "for sent in words_fa:\n",
    "    sent_arr = []\n",
    "    for word in sent:\n",
    "        sent_arr.append(normalizer.remove_specials_chars(word))\n",
    "    sents_no_punctuation_fa.append(sent_arr)\n",
    "\n",
    "for sent in words_en:\n",
    "    sent_arr = []\n",
    "    for word in sent:\n",
    "        sent_arr.append(normalizer.remove_specials_chars(word))\n",
    "    sents_no_punctuation_en.append(sent_arr)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_no_punctuation_en[110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so it seems that the prev approach is not working\n",
    "sents_without_punctuation_fa = [re.sub(r'[^\\w\\s]', '', sentence) for sentence in normalized_sentences_fa]\n",
    "sents_without_punctuation_en = [re.sub(r'[^\\w\\s]', '', sentence) for sentence in normalized_sentences_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_without_punctuation_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\" \n",
    "        u\"\\U0001F600-\\U0001F64F\" # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\" # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\" # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\" # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "sents_no_emji_fa = [remove_emoji(sentence) for sentence in sents_without_punctuation_fa]\n",
    "sents_no_emji_en = [remove_emoji(sentence) for sentence in sents_without_punctuation_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_no_emji_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_no_emji_fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import Lemmatizer, word_tokenize\n",
    "\n",
    "normalized_fa = normalizer.normalize(hp_fa)\n",
    "tokenized_fa = [word_tokenize(sentence) for sentence in sents_no_emji_fa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = Lemmatizer()\n",
    "\n",
    "lemmas = []\n",
    "for sentence in tokenized_fa:\n",
    "    arr = []\n",
    "    for token in sentence:\n",
    "        arr.append(lemmatizer.lemmatize(token))\n",
    "    lemmas.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(lemmas[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_lower_en = [sentence.lower() for sentence in sentences_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_lower_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_lower_en = [word_tokenize(sentence) for sentence in sents_lower_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'http[s]?://\\S+|www\\.\\S+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_num_url_en = [remove_numbers(remove_urls(sentence)) for sentence in sents_lower_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_num_url_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_num_url_punc_en = [re.sub(r'[^\\w\\s]', '', sentence) for sentence in no_num_url_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_num_url_punc_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_num_url_punc_text_en = \" \".join(no_num_url_punc_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(no_num_url_punc_text_en)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word to num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_numbers(text:str):\n",
    "    units = {\n",
    "        'صفر': 0, 'یک': 1, 'دو': 2, 'سه': 3, 'چهار': 4, 'پنج': 5,\n",
    "        'شش': 6, 'هفت': 7, 'هشت': 8, 'نه': 9, 'ده': 10, 'یازده': 11,\n",
    "        'دوازده': 12, 'سیزده': 13, 'چهارده': 14, 'پانزده': 15, 'شانزده': 16,\n",
    "        'هفده': 17, 'هجده': 18, 'نوزده': 19\n",
    "    }\n",
    "\n",
    "    tens = {\n",
    "        'بیست': 20, 'سی': 30, 'چهل': 40, 'پنجاه': 50,\n",
    "        'شصت': 60, 'هفتاد': 70, 'هشتاد': 80, 'نود': 90\n",
    "    }\n",
    "\n",
    "    hundreds = {\n",
    "        'صد': 100, 'دویست': 200, 'سیصد': 300, 'چهارصد': 400,\n",
    "        'پانصد': 500, 'ششصد': 600, 'هفتصد': 700, 'هشتصد': 800, 'نهصد': 900\n",
    "    }\n",
    "\n",
    "    scales = {\n",
    "        'هزار': 1000, 'میلیون': 1000000, 'میلیارد': 1000000000\n",
    "    }\n",
    "\n",
    "    words = text.split()\n",
    "    current_number = 0\n",
    "    total = 0\n",
    "\n",
    "    for word in words:\n",
    "        if word in units:\n",
    "            current_number += units[word]\n",
    "        elif word in tens:\n",
    "            current_number += tens[word]\n",
    "        elif word in hundreds:\n",
    "            current_number += hundreds[word]\n",
    "        elif word in scales:\n",
    "            current_number = current_number if current_number else 1\n",
    "            total += current_number * scales[word]\n",
    "            current_number = 0\n",
    "        elif word == 'و':\n",
    "            continue\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    total += current_number\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"پنجاه و دو هزار و سیصد و بیست و یک\" \n",
    "number = words_to_numbers(text)\n",
    "print(number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spell Check and Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = pd.read_csv(\"data/Vocabulary.txt\", header=None, names=[\"word\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import Normalizer, word_tokenize\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"According to a researchwe at an english university, it doesn't mattere in what order the letters in a word are, the only important thing is that fst and last letter is at the right place. The rewt can bee a total mess and you can stil read it without problem. This is because we do not read evry letter by ielf but the word as a whole. Cheerio.\n",
    "\"\"\"\n",
    "\n",
    "tokenized = word_tokenize(text)\n",
    "normalizer = Normalizer()\n",
    "normalized = [normalizer.normalize(word) for word in tokenized]\n",
    "no_puncs = [re.sub(r'[^\\w\\s]', '', sentence) for sentence in normalized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misspelled = [word for word in no_puncs if word not in vocabs[\"word\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in misspelled:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrections = {word: spell.correction(word) for word in misspelled}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_tokenized = tokenized\n",
    "for word, correction in corrections.items():\n",
    "    if word in tokenized:\n",
    "        corrected_tokenized[tokenized.index(word)] = correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_text = \" \".join(corrected_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bulding the func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_spelling(text):\n",
    "    tokenized = word_tokenize(text)\n",
    "    normalizer = Normalizer()\n",
    "    normalized = [normalizer.normalize(word) for word in tokenized]\n",
    "    no_puncs = [re.sub(r'[^\\w\\s]', '', sentence) for sentence in normalized]\n",
    "\n",
    "    misspelled = [word for word in no_puncs if word not in vocabs[\"word\"]]\n",
    "\n",
    "    corrections = {word: spell.correction(word) for word in misspelled}\n",
    "\n",
    "    corrected_tokenized = tokenized\n",
    "    for word, correction in corrections.items():\n",
    "        if word in tokenized:\n",
    "            corrected_tokenized[tokenized.index(word)] = correction\n",
    "\n",
    "    corrected_text = \" \".join(corrected_tokenized)\n",
    "    \n",
    "    return corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_text = correct_spelling(text)\n",
    "print(corrected_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1379aec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\CharismaStore.ir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\CharismaStore.ir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee8b140",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a0be301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.corpus.gutenberg.fileids()\n",
    "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt',\n",
    "'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt',\n",
    "'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt',\n",
    "'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt',\n",
    "'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt',\n",
    "'shakespeare-macbeth.txt', 'whitman-leaves.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "364cac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "gutenberg.fileids()\n",
    "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', ...]\n",
    "emma = gutenberg.words('austen-emma.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b88ae4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 25 26 austen-emma.txt\n",
      "5 26 17 austen-persuasion.txt\n",
      "5 28 22 austen-sense.txt\n",
      "4 34 79 bible-kjv.txt\n",
      "5 19 5 blake-poems.txt\n",
      "4 19 14 bryant-stories.txt\n",
      "4 18 12 burgess-busterbrown.txt\n",
      "4 20 13 carroll-alice.txt\n",
      "5 20 12 chesterton-ball.txt\n",
      "5 23 11 chesterton-brown.txt\n",
      "5 18 11 chesterton-thursday.txt\n",
      "4 21 25 edgeworth-parents.txt\n",
      "5 26 15 melville-moby_dick.txt\n",
      "5 52 11 milton-paradise.txt\n",
      "4 12 9 shakespeare-caesar.txt\n",
      "4 12 8 shakespeare-hamlet.txt\n",
      "4 12 7 shakespeare-macbeth.txt\n",
      "5 36 12 whitman-leaves.txt\n"
     ]
    }
   ],
   "source": [
    "for fileid in gutenberg.fileids():\n",
    "    num_chars = len(gutenberg.raw(fileid))\n",
    "    num_words = len(gutenberg.words(fileid))\n",
    "    num_sents = len(gutenberg.sents(fileid))\n",
    "    num_vocab = len(set(w.lower() for w in gutenberg.words(fileid)))\n",
    "    print(round(num_chars/num_words), round(num_words/num_sents), round(num_words/num_vocab), fileid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90db3844",
   "metadata": {},
   "source": [
    "# Main Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "322100e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\CharismaStore.ir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\CharismaStore.ir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt')  # For tokenization\n",
    "\n",
    "documents = [reuters.raw(fileid) for fileid in reuters.fileids()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ab1cc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\CharismaStore.ir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t.isalpha()]  # keep only words\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "cleaned_docs = [preprocess(doc) for doc in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "572b6915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "n = 2\n",
    "all_ngrams = []\n",
    "\n",
    "for tokens in cleaned_docs:\n",
    "    all_ngrams.extend(ngrams(tokens, n))\n",
    "\n",
    "freq_dist = Counter(all_ngrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f77eea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mln', 'dlrs'): 4476\n",
      "('vs', 'mln'): 3948\n",
      "('mln', 'vs'): 3919\n",
      "('cts', 'vs'): 3313\n",
      "('cts', 'net'): 2196\n",
      "('vs', 'cts'): 1933\n",
      "('vs', 'loss'): 1789\n",
      "('billion', 'dlrs'): 1747\n",
      "('net', 'vs'): 1579\n",
      "('revs', 'mln'): 1549\n"
     ]
    }
   ],
   "source": [
    "for gram, freq in freq_dist.most_common(10):\n",
    "    print(f\"{gram}: {freq}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

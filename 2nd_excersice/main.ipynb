{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1379aec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\CharismaStore.ir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a0be301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.corpus.gutenberg.fileids()\n",
    "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt',\n",
    "'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt',\n",
    "'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt',\n",
    "'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt',\n",
    "'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt',\n",
    "'shakespeare-macbeth.txt', 'whitman-leaves.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "364cac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "gutenberg.fileids()\n",
    "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', ...]\n",
    "emma = gutenberg.words('austen-emma.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b88ae4d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No sentence tokenizer for this corpus",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\nltk\\corpus\\reader\\plaintext.py:89\u001b[39m, in \u001b[36mPlaintextCorpusReader.sents\u001b[39m\u001b[34m(self, fileids)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     \u001b[38;5;28mself\u001b[39m._sent_tokenizer = \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1743\u001b[39m PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\CharismaStore.ir/nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.13_3.13.1008.0_x64__qbz5n2kfra8p0\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.13_3.13.1008.0_x64__qbz5n2kfra8p0\\\\share\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.13_3.13.1008.0_x64__qbz5n2kfra8p0\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\CharismaStore.ir\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m num_chars = \u001b[38;5;28mlen\u001b[39m(gutenberg.raw(fileid))\n\u001b[32m      3\u001b[39m num_words = \u001b[38;5;28mlen\u001b[39m(gutenberg.words(fileid))\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m num_sents = \u001b[38;5;28mlen\u001b[39m(\u001b[43mgutenberg\u001b[49m\u001b[43m.\u001b[49m\u001b[43msents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileid\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m      5\u001b[39m num_vocab = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(w.lower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m gutenberg.words(fileid)))\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mround\u001b[39m(num_chars/num_words), \u001b[38;5;28mround\u001b[39m(num_words/num_sents), \u001b[38;5;28mround\u001b[39m(num_words/num_vocab), fileid)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\nltk\\corpus\\reader\\plaintext.py:91\u001b[39m, in \u001b[36mPlaintextCorpusReader.sents\u001b[39m\u001b[34m(self, fileids)\u001b[39m\n\u001b[32m     89\u001b[39m         \u001b[38;5;28mself\u001b[39m._sent_tokenizer = PunktTokenizer()\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo sentence tokenizer for this corpus\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m concat(\n\u001b[32m     94\u001b[39m     [\n\u001b[32m     95\u001b[39m         \u001b[38;5;28mself\u001b[39m.CorpusView(path, \u001b[38;5;28mself\u001b[39m._read_sent_block, encoding=enc)\n\u001b[32m     96\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m (path, enc, fileid) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.abspaths(fileids, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     97\u001b[39m     ]\n\u001b[32m     98\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: No sentence tokenizer for this corpus"
     ]
    }
   ],
   "source": [
    "for fileid in gutenberg.fileids():\n",
    "    num_chars = len(gutenberg.raw(fileid))\n",
    "    num_words = len(gutenberg.words(fileid))\n",
    "    num_sents = len(gutenberg.sents(fileid))\n",
    "    num_vocab = len(set(w.lower() for w in gutenberg.words(fileid)))\n",
    "    print(round(num_chars/num_words), round(num_words/num_sents), round(num_words/num_vocab), fileid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# Text Preprocessing

This section we learn about text preprocessing, including normalization etc.

## What is Text?

- Characters (e.g., a, b, 1, !)
- Words
- Phrases and name entities
- Sentences
- Paragraphs

## Tokenization

Tokenization is the process of splitting input into smaller parts called tokens.

## What is a Word? Where is the Word Boundary?

- A word is a meaningful sequence of characters.
- Key question: How do we find word boundaries?

- Word boundaries vary across languages.
- In English and Persian, spaces and punctuation often help.
  - Example: spaces ( ), commas (,), periods (.)


## Common Word Forms

- There are two common definitions for a word:

### Word Type (Type)
- The **unique** entries in the vocabulary (e.g., how many distinct words exist?).

### Word Token (Token)
- The actual occurrences of those words (e.g., how many total words appear? even repeated ones).


## Example: Tokens vs. Types

Sentence:
They lay back on the San Francisco grass and looked at the stars and their

- How many tokens? â†’ 14 or 15?
- How many types (unique words)? â†’ 11, 12, or 13?


## Tokens and Types in Real Datasets

| Dataset                        | Tokens         | Types (Vocabulary) |
|-------------------------------|----------------|---------------------|
| Switchboard phone conversations | 2.4 million    | 20,000              |
| Shakespeare                    | 884,000        | 31,000              |
| Google N-grams                 | 1 trillion     | 13 million          |

- N = number of tokens  
- V = set of unique word types  
- |V| = vocabulary size

ğŸ“Œ Church and Gale (1990):  
Vocabulary size grows slower than number of tokens:  
**|V| > O(âˆšN)**


## **Important**: Should We Treat Punctuation as Separate Tokens?

- It depends on the task and use case.

- In tasks like sentiment analysis, punctuation (like "!" or "?") can be important.

Example:
â€œI hate dogs.â€  
â€œI hate dogs?â€

â†’ These two sentences have different meanings.

â†’ In tokenization, punctuation may be kept:
["I", "hate", "dogs", "?"]

âš ï¸ Some libraries like `CountVectorizer` in scikit-learn remove punctuation by default during tokenization.

## Character-Based Tokenization (Simple)

Instead of breaking text into words, we can break it into characters.

Example:
"dog" â†’ ["d", "o", "g"]

### Why do this?

âœ… Pros:
- We donâ€™t need to save a big word list.
- It helps when we see new or weird words.
- It works well in deep learning models.

âŒ Cons:
- Each word becomes many tokens â†’ longer input.
- Characters donâ€™t have much meaning alone.
- Itâ€™s harder for the model to learn from long sequences of characters.

## Slide 18 â€“ Character vs. Word Tokens

âœ… Word tokens have more meaning.

Example: The word "Ú©Ø¨ÙˆØªØ±" (pigeon) gives a clear image or meaning.

âŒ Character tokens have less meaning.

Example: The character "d" appears in "dog", "door", and "duck" â€” but on its own, it doesnâ€™t mean much.

â†’ So, word-based tokens are usually more meaningful than character-based ones.

## Slide 19 â€“ Why Use Character Tokens?

âœ… Number of characters is small.
- English has only 26 letters + some punctuation.

âœ… Vocabulary made of characters is very small.
- Easier to store and process.

âœ… Computers can handle character-based data more easily and with less memory.

## Slide 20 â€“ Subword Tokenization

Subword tokens are **parts of words**.

Example:
- "eating" â†’ ["eat", "ing"]

Why this helps:
- "eat" and "eating" are similar in meaning.
- If we use subword tokens, the model learns they are related.
- If not, it sees them as totally different words.

â†’ Better for machine learning models to represent similar words together.

## Slide 21 â€“ Why Subword Tokenization Matters

If two words like "walk" and "walking" are split into subwords, the model learns they are related.

If not, it may treat them as completely different.

ğŸ“Œ Important question:
Should "walk", "walking", "walked", and "walks" all be learned separately?
Or should the model learn their shared meaning?

â†’ Deep learning models (like Transformers) help answer this with better word representation.

## Slide 22 â€“ Final Note on Tokenization

- Natural language has **lots of exceptions**.
- Tokenization is hard to model with fixed rules.

Thatâ€™s why:
- Traditional rule-based methods canâ€™t handle all cases.
- We use machine learning to **learn** tokenization from real data.

ğŸ“Œ ML is needed because languages are full of messy exceptions.

## Slide 23 â€“ Stopwords

**Stopwords** are very common words that appear a lot in text but donâ€™t carry much meaning.

Examples:

- English: â€œtheâ€, â€œandâ€, â€œorâ€, â€œisâ€, â€œitâ€
- Persian: "Ùˆ", "Ø¯Ø±", "ÛŒØ§", "Ø¢Ù†"

ğŸŸ¡ They are usually:
- Pronouns, prepositions, conjunctions, helper verbs, etc.

ğŸ›  No universal list of stopwords exists â€” each tool may have its own.

## Slide 24 â€“ Why Remove Stopwords?

âœ… Useful in many cases:

- Helps reduce the size of the text
- Makes important words stand out more
- Improves accuracy for tasks like text classification

âš ï¸ But not always good to remove:

- Some stopwords may be important depending on the task

## Slide 25 â€“ When Removing Stopwords is Bad

âŒ In tasks like:

- **Machine translation**: stopwords need to be translated
- **Question answering**: removing them might change meaning
- **Text summarization**: affects grammar and structure

Example:

Original: â€œThe movie was **not good** at allâ€  
After removing stopwords: â€œmovie goodâ€

â†’ Meaning is completely changed!

## Slide 26 â€“ Libraries with Stopword Lists

Popular NLP libraries that provide stopword lists:

- **NLTK**
- **spaCy**
- **Gensim**
- **Scikit-learn**

âš ï¸ Be careful: Removing stopwords blindly can change meaning!


## Important: Word Nomalizati0on and Stemming (27)

**Text normalization** = Converting words into a standard form.

Why? Because the same word can appear in many different ways:

Examples:
- "labeled" vs. "labelled"
- "extra-linguistic" vs. "extralinguistic"

â†’ Normalize them to a single standard form.

## Slide 28 â€“ What is Normalization?

Make all forms of a word the same.

Example:  
"USA" = "U.S.A" â†’ should be treated as the same.

Also:
- "window" and "windows" â†’ both should map to "window" in some tasks.

This helps in:
- Searching
- Matching
- Indexing

## Slide 29 â€“ Case Normalization

Often we convert **all letters to lowercase** for consistency.

Examples:
- "USA" vs. "us"
- "Fed" vs. "fed"
- "SAIL" vs. "sail"

âš ï¸ But case matters in some tasks like:
- Sentiment analysis
- Information extraction
- Machine translation

## Slide 30 â€“ Morphology & Word Structure

Some words are made of smaller parts (called **morphemes**):

- **Stem**: the core meaning part of a word
- **Affix**: the extra parts (prefixes/suffixes)

Example:
- In "unhappiness":  
  - "happy" = stem  
  - "un" and "ness" = affixes

â†’ Understanding word structure helps in better normalization.
